{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity (MF) optimization\n",
    "\n",
    "In most cases it is better to do many cheap evaluations of an approximation to the target function than it is to only optimize the target function. This example demonstrates the 'multi-fidelity' capabilities of xopt. \n",
    "\n",
    "We follow the implementation of multi-fidelity bayesian optimization used in botorch https://botorch.org/tutorials/multi_fidelity_bo to optimize the synthetic test function AugmentedHartmann https://botorch.org/api/test_functions.html.\n",
    "\n",
    "The difference between normal Bayesian optimization and MF optimization is that we specify a 'cost' to making observations at a given fidelity. For this example we assume a base cost of 5 and a fidelity cost between 0-1. The algorithm should make many observations at lower fidelity relative to higher fidelity, lowering the total observation cost. \n",
    "\n",
    "NOTE: The cost parameter is required to be the LAST element of the variables list. Also this method is best suited for parallel observations of the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see log messages\n",
    "from xopt import output_notebook\n",
    "output_notebook('DEBUG')\n",
    "\n",
    "# Import the class\n",
    "from xopt import Xopt\n",
    "from botorch.test_functions.multi_fidelity import AugmentedHartmann\n",
    "import os\n",
    "SMOKE_TEST = os.environ.get('SMOKE_TEST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Xopt` object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.\n",
    "\n",
    "Here we will make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a proper input file. \n",
    "import yaml\n",
    "YAML = \"\"\"\n",
    "xopt: {output_path: null}\n",
    "\n",
    "algorithm:\n",
    "  name: multi_fidelity\n",
    "  options:  \n",
    "      batch_size: 4\n",
    "      budget: 200\n",
    "      fixed_cost: 5.0\n",
    "      n_initial_samples: 16\n",
    "\n",
    "simulation: \n",
    "  name: test_multi_fidelity\n",
    "  evaluate: xopt.tests.test_functions.multi_fidelity.evaluate\n",
    "\n",
    "vocs:\n",
    "  description: null\n",
    "  variables:\n",
    "    x1: [0, 1.0]\n",
    "    x2: [0, 1.0]\n",
    "    x3: [0, 1.0]\n",
    "    x4: [0, 1.0]\n",
    "    x5: [0, 1.0]\n",
    "    x6: [0, 1.0]\n",
    "    cost: [0, 1.0]                          ## NOTE: THIS IS REQUIRED FOR MULTI-FIDELITY OPTIMIZATION\n",
    "  objectives:\n",
    "    y1: 'MINIMIZE'\n",
    "  linked_variables: {}\n",
    "  constants: {a: dummy_constant}\n",
    "\n",
    "\"\"\"\n",
    "config = yaml.safe_load(YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from dict.\n",
      "`description` keyword no longer allowed in vocs config, removing\n",
      "`evaluate key in evaluate no longer allowed, replacing with `function` keyword\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "Loading config from dict.\n",
      "Warning: No path set for key xopt : output_path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Roussel\\Documents\\GitHub\\Xopt\\xopt\\legacy.py:40: UserWarning: `simulation` keyword no longer allowed in evaluate config, moving to `evaluate`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\n            Xopt \n________________________________           \nVersion: 0.5.0+16.g28935f4.dirty\nConfigured: True\nConfig as YAML:\nxopt: {output_path: null}\nalgorithm:\n  name: multi_fidelity\n  options: {batch_size: 4, budget: 200, fixed_cost: 5.0, n_initial_samples: 16, output_path: .,\n    restart_file: null, num_restarts: 20, raw_samples: 1024, num_fantasies: 128}\n  type: batched\n  function: null\nvocs:\n  variables:\n    x1: [0, 1.0]\n    x2: [0, 1.0]\n    x3: [0, 1.0]\n    x4: [0, 1.0]\n    x5: [0, 1.0]\n    x6: [0, 1.0]\n    cost: [0, 1.0]\n  objectives: {y1: MINIMIZE}\n  linked_variables: {}\n  constants: {a: dummy_constant}\n  constraints: {}\nevaluate:\n  name: test_multi_fidelity\n  function: xopt.tests.test_functions.multi_fidelity.evaluate\n  executor: null\n  options: {extra_option: abc}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    config['algorithm']['options']['budget'] = 3\n",
    "    config['algorithm']['options']['processes'] = 1\n",
    "    config['algorithm']['options']['generator_options']['num_restarts'] = 2\n",
    "    config['algorithm']['options']['generator_options']['raw_samples'] = 2\n",
    "    config['algorithm']['options']['generator_options']['base_acq'] = None\n",
    "\n",
    "X = Xopt(config)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at time 2021-10-21T15:43:52-05:00\n",
      "Generating and submitting initial samples\n",
      "generating samples\n",
      "generated 4 samples in 21.59 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6      cost\n",
      "0  0.328708  0.995807  0.945844  0.549720  0.937641  0.004442  0.055705\n",
      "1  0.323865  0.010545  0.449040  0.457163  0.316815  0.822545  0.130918\n",
      "2  0.331510  0.855107  0.923964  0.725311  0.954624  0.006981  0.066421\n",
      "3  0.242447  0.815054  0.897403  0.569898  0.843244  0.002512  0.063891\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 18.6 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6      cost\n",
      "0  0.428529  0.945932  0.759510  0.478149  1.000000  0.000000  0.853596\n",
      "1  0.435890  1.000000  1.000000  0.566096  0.774466  0.132344  0.795087\n",
      "2  0.527218  0.824328  0.820093  0.660372  0.803974  0.000000  0.000000\n",
      "3  0.670458  0.019290  0.474177  0.135217  0.000000  0.842108  0.000000\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 26.55 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6      cost\n",
      "0  0.396007  0.936906  0.923960  0.618633  1.000000  0.247731  0.000000\n",
      "1  0.381114  1.000000  0.714675  0.604593  0.835919  0.030602  0.628873\n",
      "2  0.417312  1.000000  1.000000  0.395524  0.998631  0.000000  0.000000\n",
      "3  0.425526  0.700614  1.000000  0.545952  0.986823  0.028850  0.787448\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 26.81 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6  cost\n",
      "0  0.062052  0.104830  0.592461  0.590078  0.452254  0.710921   0.0\n",
      "1  0.413044  1.000000  0.820338  0.532640  0.674187  0.000000   0.0\n",
      "2  0.366621  1.000000  0.629969  0.666022  0.956616  0.000000   0.0\n",
      "3  0.032765  0.112589  0.592997  0.604469  0.483798  0.698809   0.0\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 28.25 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6      cost\n",
      "0  0.591999  0.225749  0.719615  0.422829  0.121426  0.617653  0.935440\n",
      "1  0.391277  0.797802  0.681030  0.521485  0.719520  0.047197  0.080497\n",
      "2  0.000000  0.250602  0.872253  1.000000  0.001637  1.000000  0.318764\n",
      "3  0.396457  1.000000  0.986534  0.683243  0.726597  0.000000  1.000000\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 31.26 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6  cost\n",
      "0  0.202118  0.139074  0.309408  0.678296  0.146203  0.142955   1.0\n",
      "1  0.353566  0.767631  0.978293  0.578673  0.729665  0.053515   1.0\n",
      "2  0.885677  0.388012  0.660741  0.393942  0.369446  0.674354   1.0\n",
      "3  0.891312  0.381945  0.725746  0.324279  0.455069  0.599132   1.0\n",
      "collecting results\n",
      "saving data to file\n",
      "generating samples\n",
      "generated 4 samples in 35.35 seconds\n",
      "samples\n",
      "         x1        x2        x3        x4        x5        x6  cost\n",
      "0  0.366594  0.939275  0.922987  0.563182  1.000000  0.078555   1.0\n",
      "1  0.360411  0.758025  0.846372  0.563595  0.979123  0.000000   1.0\n",
      "2  0.378437  0.749237  0.832784  0.621120  0.873227  0.069232   1.0\n",
      "3  0.792946  0.206232  0.471104  0.357287  0.248451  0.589671   1.0\n",
      "collecting results\n",
      "saving data to file\n"
     ]
    },
    {
     "data": {
      "text/plain": "          x1        x2        x3        x4        x5        x6      cost  \\\n0   0.691244  0.615999  0.871892  0.406190  0.929676  0.203446  0.258304   \n1   0.070838  0.126777  0.804781  0.633564  0.129536  0.714260  0.273388   \n2   0.145854  0.156026  0.297237  0.247525  0.076005  0.126591  0.498973   \n3   0.891793  0.493741  0.035277  0.701003  0.329419  0.407734  0.397707   \n4   0.937207  0.892452  0.358672  0.874244  0.982042  0.767259  0.508448   \n5   0.603009  0.086291  0.506407  0.289901  0.109287  0.762377  0.948699   \n6   0.078693  0.292805  0.925308  0.707332  0.068799  0.724938  0.198843   \n7   0.371620  0.845505  0.905252  0.585629  0.890036  0.034826  0.053120   \n8   0.201152  0.225770  0.969912  0.275714  0.945538  0.573256  0.370739   \n9   0.975543  0.690233  0.346602  0.621266  0.116104  0.126608  0.156477   \n10  0.413581  0.508347  0.100084  0.109395  0.739249  0.565016  0.893795   \n11  0.468681  0.897854  0.762952  0.817277  0.354742  0.720764  0.100459   \n12  0.354650  0.480501  0.900894  0.359577  0.491243  0.487551  0.090752   \n13  0.114426  0.071814  0.536838  0.551659  0.461446  0.739219  0.333155   \n14  0.832431  0.266447  0.582120  0.344168  0.158139  0.596631  0.596525   \n15  0.600664  0.124148  0.671992  0.232971  0.595919  0.266744  0.624638   \n16  0.331510  0.855107  0.923964  0.725311  0.954624  0.006981  0.066421   \n17  0.323865  0.010545  0.449040  0.457163  0.316815  0.822545  0.130918   \n18  0.328708  0.995807  0.945844  0.549720  0.937641  0.004442  0.055705   \n19  0.242447  0.815054  0.897403  0.569898  0.843244  0.002512  0.063891   \n20  0.527218  0.824328  0.820093  0.660372  0.803974  0.000000  0.000000   \n21  0.428529  0.945932  0.759510  0.478149  1.000000  0.000000  0.853596   \n22  0.435890  1.000000  1.000000  0.566096  0.774466  0.132344  0.795087   \n23  0.670458  0.019290  0.474177  0.135217  0.000000  0.842108  0.000000   \n24  0.396007  0.936906  0.923960  0.618633  1.000000  0.247731  0.000000   \n25  0.381114  1.000000  0.714675  0.604593  0.835919  0.030602  0.628873   \n26  0.425526  0.700614  1.000000  0.545952  0.986823  0.028850  0.787448   \n27  0.417312  1.000000  1.000000  0.395524  0.998631  0.000000  0.000000   \n28  0.062052  0.104830  0.592461  0.590078  0.452254  0.710921  0.000000   \n29  0.413044  1.000000  0.820338  0.532640  0.674187  0.000000  0.000000   \n30  0.366621  1.000000  0.629969  0.666022  0.956616  0.000000  0.000000   \n31  0.032765  0.112589  0.592997  0.604469  0.483798  0.698809  0.000000   \n32  0.591999  0.225749  0.719615  0.422829  0.121426  0.617653  0.935440   \n33  0.396457  1.000000  0.986534  0.683243  0.726597  0.000000  1.000000   \n34  0.000000  0.250602  0.872253  1.000000  0.001637  1.000000  0.318764   \n35  0.391277  0.797802  0.681030  0.521485  0.719520  0.047197  0.080497   \n36  0.891312  0.381945  0.725746  0.324279  0.455069  0.599132  1.000000   \n37  0.353566  0.767631  0.978293  0.578673  0.729665  0.053515  1.000000   \n38  0.885677  0.388012  0.660741  0.393942  0.369446  0.674354  1.000000   \n39  0.202118  0.139074  0.309408  0.678296  0.146203  0.142955  1.000000   \n40  0.360411  0.758025  0.846372  0.563595  0.979123  0.000000  1.000000   \n41  0.366594  0.939275  0.922987  0.563182  1.000000  0.078555  1.000000   \n42  0.792946  0.206232  0.471104  0.357287  0.248451  0.589671  1.000000   \n43  0.378437  0.749237  0.832784  0.621120  0.873227  0.069232  1.000000   \n\n                 a        y1 status      x1_t      x2_t      x3_t      x4_t  \\\n0   dummy_constant -0.216302   done  0.691244  0.615999  0.871892  0.406190   \n1   dummy_constant -0.541040   done  0.070838  0.126777  0.804781  0.633564   \n2   dummy_constant -0.137570   done  0.145854  0.156026  0.297237  0.247525   \n3   dummy_constant -0.050089   done  0.891793  0.493741  0.035277  0.701003   \n4   dummy_constant -0.000018   done  0.937207  0.892452  0.358672  0.874244   \n5   dummy_constant -0.974108   done  0.603009  0.086291  0.506407  0.289901   \n6   dummy_constant -0.406239   done  0.078693  0.292805  0.925308  0.707332   \n7   dummy_constant -2.918608   done  0.371620  0.845505  0.905252  0.585629   \n8   dummy_constant -0.039041   done  0.201152  0.225770  0.969912  0.275714   \n9   dummy_constant -0.011705   done  0.975543  0.690233  0.346602  0.621266   \n10  dummy_constant -0.052635   done  0.413581  0.508347  0.100084  0.109395   \n11  dummy_constant -0.040095   done  0.468681  0.897854  0.762952  0.817277   \n12  dummy_constant -0.541420   done  0.354650  0.480501  0.900894  0.359577   \n13  dummy_constant -1.064931   done  0.114426  0.071814  0.536838  0.551659   \n14  dummy_constant -0.613746   done  0.832431  0.266447  0.582120  0.344168   \n15  dummy_constant -0.139433   done  0.600664  0.124148  0.671992  0.232971   \n16  dummy_constant -2.122951   done  0.331510  0.855107  0.923964  0.725311   \n17  dummy_constant -1.772141   done  0.323865  0.010545  0.449040  0.457163   \n18  dummy_constant -2.391836   done  0.328708  0.995807  0.945844  0.549720   \n19  dummy_constant -1.835717   done  0.242447  0.815054  0.897403  0.569898   \n20  dummy_constant -2.091561   done  0.527218  0.824328  0.820093  0.660372   \n21  dummy_constant -2.532454   done  0.428529  0.945932  0.759510  0.478149   \n22  dummy_constant -2.379107   done  0.435890  1.000000  1.000000  0.566096   \n23  dummy_constant -0.224905   done  0.670458  0.019290  0.474177  0.135217   \n24  dummy_constant -1.528963   done  0.396007  0.936906  0.923960  0.618633   \n25  dummy_constant -2.664875   done  0.381114  1.000000  0.714675  0.604593   \n26  dummy_constant -2.232781   done  0.425526  0.700614  1.000000  0.545952   \n27  dummy_constant -1.878989   done  0.417312  1.000000  1.000000  0.395524   \n28  dummy_constant -0.875682   done  0.062052  0.104830  0.592461  0.590078   \n29  dummy_constant -2.671269   done  0.413044  1.000000  0.820338  0.532640   \n30  dummy_constant -2.338999   done  0.366621  1.000000  0.629969  0.666022   \n31  dummy_constant -0.694672   done  0.032765  0.112589  0.592997  0.604469   \n32  dummy_constant -0.842498   done  0.591999  0.225749  0.719615  0.422829   \n33  dummy_constant -2.397331   done  0.396457  1.000000  0.986534  0.683243   \n34  dummy_constant -0.793853   done  0.000000  0.250602  0.872253  1.000000   \n35  dummy_constant -2.817549   done  0.391277  0.797802  0.681030  0.521485   \n36  dummy_constant -0.384966   done  0.891312  0.381945  0.725746  0.324279   \n37  dummy_constant -2.638682   done  0.353566  0.767631  0.978293  0.578673   \n38  dummy_constant -0.577117   done  0.885677  0.388012  0.660741  0.393942   \n39  dummy_constant -0.069027   done  0.202118  0.139074  0.309408  0.678296   \n40  dummy_constant -2.480546   done  0.360411  0.758025  0.846372  0.563595   \n41  dummy_constant -2.744108   done  0.366594  0.939275  0.922987  0.563182   \n42  dummy_constant -0.991347   done  0.792946  0.206232  0.471104  0.357287   \n43  dummy_constant -2.497908   done  0.378437  0.749237  0.832784  0.621120   \n\n        x5_t      x6_t    cost_t      y1_t  \n0   0.929676  0.203446  0.258304  0.365187  \n1   0.129536  0.714260  0.273388 -0.077505  \n2   0.076005  0.126591  0.498973  0.472516  \n3   0.329419  0.407734  0.397707  0.591772  \n4   0.982042  0.767259  0.508448  0.660031  \n5   0.109287  0.762377  0.948699 -0.667874  \n6   0.068799  0.724938  0.198843  0.106260  \n7   0.890036  0.034826  0.053120 -3.318665  \n8   0.945538  0.573256  0.370739  0.606834  \n9   0.116104  0.126608  0.156477  0.644098  \n10  0.739249  0.565016  0.893795  0.588302  \n11  0.354742  0.720764  0.100459  0.605396  \n12  0.491243  0.487551  0.090752 -0.078023  \n13  0.461446  0.739219  0.333155 -0.791686  \n14  0.158139  0.596631  0.596525 -0.176619  \n15  0.595919  0.266744  0.624638  0.469976  \n16  0.954624  0.006981  0.066421 -0.323739  \n17  0.316815  0.822545  0.130918  0.906856  \n18  0.937641  0.004442  0.055705 -1.266954  \n19  0.843244  0.002512  0.063891  0.683837  \n20  0.803974  0.000000  0.000000 -0.265827  \n21  1.000000  0.000000  0.853596 -0.677703  \n22  0.774466  0.132344  0.795087 -0.534448  \n23  0.000000  0.842108  0.000000  1.477978  \n24  1.000000  0.247731  0.000000  1.125760  \n25  0.835919  0.030602  0.628873 -1.210142  \n26  0.986823  0.028850  0.787448 -0.321579  \n27  0.998631  0.000000  0.000000  0.405963  \n28  0.452254  0.710921  0.000000  0.765676  \n29  0.674187  0.000000  0.000000 -1.021049  \n30  0.956616  0.000000  0.000000 -0.690419  \n31  0.483798  0.698809  0.000000  0.945792  \n32  0.121426  0.617653  0.935440  0.830949  \n33  0.726597  0.000000  1.000000 -0.653565  \n34  0.001637  1.000000  0.318764  0.877394  \n35  0.719520  0.047197  0.080497 -1.054778  \n36  0.455069  0.599132  1.000000  0.456498  \n37  0.729665  0.053515  1.000000 -1.475616  \n38  0.369446  0.674354  1.000000  0.291766  \n39  0.146203  0.142955  1.000000  0.727352  \n40  0.979123  0.000000  1.000000 -0.377340  \n41  1.000000  0.078555  1.000000 -0.706579  \n42  0.248451  0.589671  1.000000  1.482948  \n43  0.873227  0.069232  1.000000 -0.399030  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>cost</th>\n      <th>a</th>\n      <th>y1</th>\n      <th>status</th>\n      <th>x1_t</th>\n      <th>x2_t</th>\n      <th>x3_t</th>\n      <th>x4_t</th>\n      <th>x5_t</th>\n      <th>x6_t</th>\n      <th>cost_t</th>\n      <th>y1_t</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.691244</td>\n      <td>0.615999</td>\n      <td>0.871892</td>\n      <td>0.406190</td>\n      <td>0.929676</td>\n      <td>0.203446</td>\n      <td>0.258304</td>\n      <td>dummy_constant</td>\n      <td>-0.216302</td>\n      <td>done</td>\n      <td>0.691244</td>\n      <td>0.615999</td>\n      <td>0.871892</td>\n      <td>0.406190</td>\n      <td>0.929676</td>\n      <td>0.203446</td>\n      <td>0.258304</td>\n      <td>0.365187</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.070838</td>\n      <td>0.126777</td>\n      <td>0.804781</td>\n      <td>0.633564</td>\n      <td>0.129536</td>\n      <td>0.714260</td>\n      <td>0.273388</td>\n      <td>dummy_constant</td>\n      <td>-0.541040</td>\n      <td>done</td>\n      <td>0.070838</td>\n      <td>0.126777</td>\n      <td>0.804781</td>\n      <td>0.633564</td>\n      <td>0.129536</td>\n      <td>0.714260</td>\n      <td>0.273388</td>\n      <td>-0.077505</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.145854</td>\n      <td>0.156026</td>\n      <td>0.297237</td>\n      <td>0.247525</td>\n      <td>0.076005</td>\n      <td>0.126591</td>\n      <td>0.498973</td>\n      <td>dummy_constant</td>\n      <td>-0.137570</td>\n      <td>done</td>\n      <td>0.145854</td>\n      <td>0.156026</td>\n      <td>0.297237</td>\n      <td>0.247525</td>\n      <td>0.076005</td>\n      <td>0.126591</td>\n      <td>0.498973</td>\n      <td>0.472516</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.891793</td>\n      <td>0.493741</td>\n      <td>0.035277</td>\n      <td>0.701003</td>\n      <td>0.329419</td>\n      <td>0.407734</td>\n      <td>0.397707</td>\n      <td>dummy_constant</td>\n      <td>-0.050089</td>\n      <td>done</td>\n      <td>0.891793</td>\n      <td>0.493741</td>\n      <td>0.035277</td>\n      <td>0.701003</td>\n      <td>0.329419</td>\n      <td>0.407734</td>\n      <td>0.397707</td>\n      <td>0.591772</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.937207</td>\n      <td>0.892452</td>\n      <td>0.358672</td>\n      <td>0.874244</td>\n      <td>0.982042</td>\n      <td>0.767259</td>\n      <td>0.508448</td>\n      <td>dummy_constant</td>\n      <td>-0.000018</td>\n      <td>done</td>\n      <td>0.937207</td>\n      <td>0.892452</td>\n      <td>0.358672</td>\n      <td>0.874244</td>\n      <td>0.982042</td>\n      <td>0.767259</td>\n      <td>0.508448</td>\n      <td>0.660031</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.603009</td>\n      <td>0.086291</td>\n      <td>0.506407</td>\n      <td>0.289901</td>\n      <td>0.109287</td>\n      <td>0.762377</td>\n      <td>0.948699</td>\n      <td>dummy_constant</td>\n      <td>-0.974108</td>\n      <td>done</td>\n      <td>0.603009</td>\n      <td>0.086291</td>\n      <td>0.506407</td>\n      <td>0.289901</td>\n      <td>0.109287</td>\n      <td>0.762377</td>\n      <td>0.948699</td>\n      <td>-0.667874</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.078693</td>\n      <td>0.292805</td>\n      <td>0.925308</td>\n      <td>0.707332</td>\n      <td>0.068799</td>\n      <td>0.724938</td>\n      <td>0.198843</td>\n      <td>dummy_constant</td>\n      <td>-0.406239</td>\n      <td>done</td>\n      <td>0.078693</td>\n      <td>0.292805</td>\n      <td>0.925308</td>\n      <td>0.707332</td>\n      <td>0.068799</td>\n      <td>0.724938</td>\n      <td>0.198843</td>\n      <td>0.106260</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.371620</td>\n      <td>0.845505</td>\n      <td>0.905252</td>\n      <td>0.585629</td>\n      <td>0.890036</td>\n      <td>0.034826</td>\n      <td>0.053120</td>\n      <td>dummy_constant</td>\n      <td>-2.918608</td>\n      <td>done</td>\n      <td>0.371620</td>\n      <td>0.845505</td>\n      <td>0.905252</td>\n      <td>0.585629</td>\n      <td>0.890036</td>\n      <td>0.034826</td>\n      <td>0.053120</td>\n      <td>-3.318665</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.201152</td>\n      <td>0.225770</td>\n      <td>0.969912</td>\n      <td>0.275714</td>\n      <td>0.945538</td>\n      <td>0.573256</td>\n      <td>0.370739</td>\n      <td>dummy_constant</td>\n      <td>-0.039041</td>\n      <td>done</td>\n      <td>0.201152</td>\n      <td>0.225770</td>\n      <td>0.969912</td>\n      <td>0.275714</td>\n      <td>0.945538</td>\n      <td>0.573256</td>\n      <td>0.370739</td>\n      <td>0.606834</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.975543</td>\n      <td>0.690233</td>\n      <td>0.346602</td>\n      <td>0.621266</td>\n      <td>0.116104</td>\n      <td>0.126608</td>\n      <td>0.156477</td>\n      <td>dummy_constant</td>\n      <td>-0.011705</td>\n      <td>done</td>\n      <td>0.975543</td>\n      <td>0.690233</td>\n      <td>0.346602</td>\n      <td>0.621266</td>\n      <td>0.116104</td>\n      <td>0.126608</td>\n      <td>0.156477</td>\n      <td>0.644098</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.413581</td>\n      <td>0.508347</td>\n      <td>0.100084</td>\n      <td>0.109395</td>\n      <td>0.739249</td>\n      <td>0.565016</td>\n      <td>0.893795</td>\n      <td>dummy_constant</td>\n      <td>-0.052635</td>\n      <td>done</td>\n      <td>0.413581</td>\n      <td>0.508347</td>\n      <td>0.100084</td>\n      <td>0.109395</td>\n      <td>0.739249</td>\n      <td>0.565016</td>\n      <td>0.893795</td>\n      <td>0.588302</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.468681</td>\n      <td>0.897854</td>\n      <td>0.762952</td>\n      <td>0.817277</td>\n      <td>0.354742</td>\n      <td>0.720764</td>\n      <td>0.100459</td>\n      <td>dummy_constant</td>\n      <td>-0.040095</td>\n      <td>done</td>\n      <td>0.468681</td>\n      <td>0.897854</td>\n      <td>0.762952</td>\n      <td>0.817277</td>\n      <td>0.354742</td>\n      <td>0.720764</td>\n      <td>0.100459</td>\n      <td>0.605396</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.354650</td>\n      <td>0.480501</td>\n      <td>0.900894</td>\n      <td>0.359577</td>\n      <td>0.491243</td>\n      <td>0.487551</td>\n      <td>0.090752</td>\n      <td>dummy_constant</td>\n      <td>-0.541420</td>\n      <td>done</td>\n      <td>0.354650</td>\n      <td>0.480501</td>\n      <td>0.900894</td>\n      <td>0.359577</td>\n      <td>0.491243</td>\n      <td>0.487551</td>\n      <td>0.090752</td>\n      <td>-0.078023</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.114426</td>\n      <td>0.071814</td>\n      <td>0.536838</td>\n      <td>0.551659</td>\n      <td>0.461446</td>\n      <td>0.739219</td>\n      <td>0.333155</td>\n      <td>dummy_constant</td>\n      <td>-1.064931</td>\n      <td>done</td>\n      <td>0.114426</td>\n      <td>0.071814</td>\n      <td>0.536838</td>\n      <td>0.551659</td>\n      <td>0.461446</td>\n      <td>0.739219</td>\n      <td>0.333155</td>\n      <td>-0.791686</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.832431</td>\n      <td>0.266447</td>\n      <td>0.582120</td>\n      <td>0.344168</td>\n      <td>0.158139</td>\n      <td>0.596631</td>\n      <td>0.596525</td>\n      <td>dummy_constant</td>\n      <td>-0.613746</td>\n      <td>done</td>\n      <td>0.832431</td>\n      <td>0.266447</td>\n      <td>0.582120</td>\n      <td>0.344168</td>\n      <td>0.158139</td>\n      <td>0.596631</td>\n      <td>0.596525</td>\n      <td>-0.176619</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.600664</td>\n      <td>0.124148</td>\n      <td>0.671992</td>\n      <td>0.232971</td>\n      <td>0.595919</td>\n      <td>0.266744</td>\n      <td>0.624638</td>\n      <td>dummy_constant</td>\n      <td>-0.139433</td>\n      <td>done</td>\n      <td>0.600664</td>\n      <td>0.124148</td>\n      <td>0.671992</td>\n      <td>0.232971</td>\n      <td>0.595919</td>\n      <td>0.266744</td>\n      <td>0.624638</td>\n      <td>0.469976</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.331510</td>\n      <td>0.855107</td>\n      <td>0.923964</td>\n      <td>0.725311</td>\n      <td>0.954624</td>\n      <td>0.006981</td>\n      <td>0.066421</td>\n      <td>dummy_constant</td>\n      <td>-2.122951</td>\n      <td>done</td>\n      <td>0.331510</td>\n      <td>0.855107</td>\n      <td>0.923964</td>\n      <td>0.725311</td>\n      <td>0.954624</td>\n      <td>0.006981</td>\n      <td>0.066421</td>\n      <td>-0.323739</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.323865</td>\n      <td>0.010545</td>\n      <td>0.449040</td>\n      <td>0.457163</td>\n      <td>0.316815</td>\n      <td>0.822545</td>\n      <td>0.130918</td>\n      <td>dummy_constant</td>\n      <td>-1.772141</td>\n      <td>done</td>\n      <td>0.323865</td>\n      <td>0.010545</td>\n      <td>0.449040</td>\n      <td>0.457163</td>\n      <td>0.316815</td>\n      <td>0.822545</td>\n      <td>0.130918</td>\n      <td>0.906856</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.328708</td>\n      <td>0.995807</td>\n      <td>0.945844</td>\n      <td>0.549720</td>\n      <td>0.937641</td>\n      <td>0.004442</td>\n      <td>0.055705</td>\n      <td>dummy_constant</td>\n      <td>-2.391836</td>\n      <td>done</td>\n      <td>0.328708</td>\n      <td>0.995807</td>\n      <td>0.945844</td>\n      <td>0.549720</td>\n      <td>0.937641</td>\n      <td>0.004442</td>\n      <td>0.055705</td>\n      <td>-1.266954</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.242447</td>\n      <td>0.815054</td>\n      <td>0.897403</td>\n      <td>0.569898</td>\n      <td>0.843244</td>\n      <td>0.002512</td>\n      <td>0.063891</td>\n      <td>dummy_constant</td>\n      <td>-1.835717</td>\n      <td>done</td>\n      <td>0.242447</td>\n      <td>0.815054</td>\n      <td>0.897403</td>\n      <td>0.569898</td>\n      <td>0.843244</td>\n      <td>0.002512</td>\n      <td>0.063891</td>\n      <td>0.683837</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.527218</td>\n      <td>0.824328</td>\n      <td>0.820093</td>\n      <td>0.660372</td>\n      <td>0.803974</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.091561</td>\n      <td>done</td>\n      <td>0.527218</td>\n      <td>0.824328</td>\n      <td>0.820093</td>\n      <td>0.660372</td>\n      <td>0.803974</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.265827</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.428529</td>\n      <td>0.945932</td>\n      <td>0.759510</td>\n      <td>0.478149</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.853596</td>\n      <td>dummy_constant</td>\n      <td>-2.532454</td>\n      <td>done</td>\n      <td>0.428529</td>\n      <td>0.945932</td>\n      <td>0.759510</td>\n      <td>0.478149</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.853596</td>\n      <td>-0.677703</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.435890</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.566096</td>\n      <td>0.774466</td>\n      <td>0.132344</td>\n      <td>0.795087</td>\n      <td>dummy_constant</td>\n      <td>-2.379107</td>\n      <td>done</td>\n      <td>0.435890</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.566096</td>\n      <td>0.774466</td>\n      <td>0.132344</td>\n      <td>0.795087</td>\n      <td>-0.534448</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.670458</td>\n      <td>0.019290</td>\n      <td>0.474177</td>\n      <td>0.135217</td>\n      <td>0.000000</td>\n      <td>0.842108</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.224905</td>\n      <td>done</td>\n      <td>0.670458</td>\n      <td>0.019290</td>\n      <td>0.474177</td>\n      <td>0.135217</td>\n      <td>0.000000</td>\n      <td>0.842108</td>\n      <td>0.000000</td>\n      <td>1.477978</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.396007</td>\n      <td>0.936906</td>\n      <td>0.923960</td>\n      <td>0.618633</td>\n      <td>1.000000</td>\n      <td>0.247731</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-1.528963</td>\n      <td>done</td>\n      <td>0.396007</td>\n      <td>0.936906</td>\n      <td>0.923960</td>\n      <td>0.618633</td>\n      <td>1.000000</td>\n      <td>0.247731</td>\n      <td>0.000000</td>\n      <td>1.125760</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.381114</td>\n      <td>1.000000</td>\n      <td>0.714675</td>\n      <td>0.604593</td>\n      <td>0.835919</td>\n      <td>0.030602</td>\n      <td>0.628873</td>\n      <td>dummy_constant</td>\n      <td>-2.664875</td>\n      <td>done</td>\n      <td>0.381114</td>\n      <td>1.000000</td>\n      <td>0.714675</td>\n      <td>0.604593</td>\n      <td>0.835919</td>\n      <td>0.030602</td>\n      <td>0.628873</td>\n      <td>-1.210142</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.425526</td>\n      <td>0.700614</td>\n      <td>1.000000</td>\n      <td>0.545952</td>\n      <td>0.986823</td>\n      <td>0.028850</td>\n      <td>0.787448</td>\n      <td>dummy_constant</td>\n      <td>-2.232781</td>\n      <td>done</td>\n      <td>0.425526</td>\n      <td>0.700614</td>\n      <td>1.000000</td>\n      <td>0.545952</td>\n      <td>0.986823</td>\n      <td>0.028850</td>\n      <td>0.787448</td>\n      <td>-0.321579</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.417312</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.395524</td>\n      <td>0.998631</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-1.878989</td>\n      <td>done</td>\n      <td>0.417312</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.395524</td>\n      <td>0.998631</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.405963</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.062052</td>\n      <td>0.104830</td>\n      <td>0.592461</td>\n      <td>0.590078</td>\n      <td>0.452254</td>\n      <td>0.710921</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.875682</td>\n      <td>done</td>\n      <td>0.062052</td>\n      <td>0.104830</td>\n      <td>0.592461</td>\n      <td>0.590078</td>\n      <td>0.452254</td>\n      <td>0.710921</td>\n      <td>0.000000</td>\n      <td>0.765676</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.413044</td>\n      <td>1.000000</td>\n      <td>0.820338</td>\n      <td>0.532640</td>\n      <td>0.674187</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.671269</td>\n      <td>done</td>\n      <td>0.413044</td>\n      <td>1.000000</td>\n      <td>0.820338</td>\n      <td>0.532640</td>\n      <td>0.674187</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.021049</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.366621</td>\n      <td>1.000000</td>\n      <td>0.629969</td>\n      <td>0.666022</td>\n      <td>0.956616</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.338999</td>\n      <td>done</td>\n      <td>0.366621</td>\n      <td>1.000000</td>\n      <td>0.629969</td>\n      <td>0.666022</td>\n      <td>0.956616</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.690419</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.032765</td>\n      <td>0.112589</td>\n      <td>0.592997</td>\n      <td>0.604469</td>\n      <td>0.483798</td>\n      <td>0.698809</td>\n      <td>0.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.694672</td>\n      <td>done</td>\n      <td>0.032765</td>\n      <td>0.112589</td>\n      <td>0.592997</td>\n      <td>0.604469</td>\n      <td>0.483798</td>\n      <td>0.698809</td>\n      <td>0.000000</td>\n      <td>0.945792</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.591999</td>\n      <td>0.225749</td>\n      <td>0.719615</td>\n      <td>0.422829</td>\n      <td>0.121426</td>\n      <td>0.617653</td>\n      <td>0.935440</td>\n      <td>dummy_constant</td>\n      <td>-0.842498</td>\n      <td>done</td>\n      <td>0.591999</td>\n      <td>0.225749</td>\n      <td>0.719615</td>\n      <td>0.422829</td>\n      <td>0.121426</td>\n      <td>0.617653</td>\n      <td>0.935440</td>\n      <td>0.830949</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.396457</td>\n      <td>1.000000</td>\n      <td>0.986534</td>\n      <td>0.683243</td>\n      <td>0.726597</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.397331</td>\n      <td>done</td>\n      <td>0.396457</td>\n      <td>1.000000</td>\n      <td>0.986534</td>\n      <td>0.683243</td>\n      <td>0.726597</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.653565</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000000</td>\n      <td>0.250602</td>\n      <td>0.872253</td>\n      <td>1.000000</td>\n      <td>0.001637</td>\n      <td>1.000000</td>\n      <td>0.318764</td>\n      <td>dummy_constant</td>\n      <td>-0.793853</td>\n      <td>done</td>\n      <td>0.000000</td>\n      <td>0.250602</td>\n      <td>0.872253</td>\n      <td>1.000000</td>\n      <td>0.001637</td>\n      <td>1.000000</td>\n      <td>0.318764</td>\n      <td>0.877394</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.391277</td>\n      <td>0.797802</td>\n      <td>0.681030</td>\n      <td>0.521485</td>\n      <td>0.719520</td>\n      <td>0.047197</td>\n      <td>0.080497</td>\n      <td>dummy_constant</td>\n      <td>-2.817549</td>\n      <td>done</td>\n      <td>0.391277</td>\n      <td>0.797802</td>\n      <td>0.681030</td>\n      <td>0.521485</td>\n      <td>0.719520</td>\n      <td>0.047197</td>\n      <td>0.080497</td>\n      <td>-1.054778</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.891312</td>\n      <td>0.381945</td>\n      <td>0.725746</td>\n      <td>0.324279</td>\n      <td>0.455069</td>\n      <td>0.599132</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.384966</td>\n      <td>done</td>\n      <td>0.891312</td>\n      <td>0.381945</td>\n      <td>0.725746</td>\n      <td>0.324279</td>\n      <td>0.455069</td>\n      <td>0.599132</td>\n      <td>1.000000</td>\n      <td>0.456498</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.353566</td>\n      <td>0.767631</td>\n      <td>0.978293</td>\n      <td>0.578673</td>\n      <td>0.729665</td>\n      <td>0.053515</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.638682</td>\n      <td>done</td>\n      <td>0.353566</td>\n      <td>0.767631</td>\n      <td>0.978293</td>\n      <td>0.578673</td>\n      <td>0.729665</td>\n      <td>0.053515</td>\n      <td>1.000000</td>\n      <td>-1.475616</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.885677</td>\n      <td>0.388012</td>\n      <td>0.660741</td>\n      <td>0.393942</td>\n      <td>0.369446</td>\n      <td>0.674354</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.577117</td>\n      <td>done</td>\n      <td>0.885677</td>\n      <td>0.388012</td>\n      <td>0.660741</td>\n      <td>0.393942</td>\n      <td>0.369446</td>\n      <td>0.674354</td>\n      <td>1.000000</td>\n      <td>0.291766</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.202118</td>\n      <td>0.139074</td>\n      <td>0.309408</td>\n      <td>0.678296</td>\n      <td>0.146203</td>\n      <td>0.142955</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.069027</td>\n      <td>done</td>\n      <td>0.202118</td>\n      <td>0.139074</td>\n      <td>0.309408</td>\n      <td>0.678296</td>\n      <td>0.146203</td>\n      <td>0.142955</td>\n      <td>1.000000</td>\n      <td>0.727352</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.360411</td>\n      <td>0.758025</td>\n      <td>0.846372</td>\n      <td>0.563595</td>\n      <td>0.979123</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.480546</td>\n      <td>done</td>\n      <td>0.360411</td>\n      <td>0.758025</td>\n      <td>0.846372</td>\n      <td>0.563595</td>\n      <td>0.979123</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.377340</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.366594</td>\n      <td>0.939275</td>\n      <td>0.922987</td>\n      <td>0.563182</td>\n      <td>1.000000</td>\n      <td>0.078555</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.744108</td>\n      <td>done</td>\n      <td>0.366594</td>\n      <td>0.939275</td>\n      <td>0.922987</td>\n      <td>0.563182</td>\n      <td>1.000000</td>\n      <td>0.078555</td>\n      <td>1.000000</td>\n      <td>-0.706579</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.792946</td>\n      <td>0.206232</td>\n      <td>0.471104</td>\n      <td>0.357287</td>\n      <td>0.248451</td>\n      <td>0.589671</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-0.991347</td>\n      <td>done</td>\n      <td>0.792946</td>\n      <td>0.206232</td>\n      <td>0.471104</td>\n      <td>0.357287</td>\n      <td>0.248451</td>\n      <td>0.589671</td>\n      <td>1.000000</td>\n      <td>1.482948</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.378437</td>\n      <td>0.749237</td>\n      <td>0.832784</td>\n      <td>0.621120</td>\n      <td>0.873227</td>\n      <td>0.069232</td>\n      <td>1.000000</td>\n      <td>dummy_constant</td>\n      <td>-2.497908</td>\n      <td>done</td>\n      <td>0.378437</td>\n      <td>0.749237</td>\n      <td>0.832784</td>\n      <td>0.621120</td>\n      <td>0.873227</td>\n      <td>0.069232</td>\n      <td>1.000000</td>\n      <td>-0.399030</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change max generations\n",
    "X.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get highest fidelity global optimum\n",
    "\n",
    "NOTE: the correct global minimum is -3.32237\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3652,  0.0775, -0.4725, -0.5918, -0.6600,  0.6679, -0.1063,  3.3187,\n",
      "        -0.6068, -0.6441, -0.5883, -0.6054,  0.0780,  0.7917,  0.1766, -0.4700,\n",
      "         0.3237, -0.9069,  1.2670, -0.6838,  0.2658,  0.6777,  0.5344, -1.4780,\n",
      "        -1.1258,  1.2101,  0.3216, -0.4060, -0.7657,  1.0210,  0.6904, -0.9458,\n",
      "        -0.8309,  0.6536, -0.8774,  1.0548, -0.4565,  1.4756, -0.2918, -0.7274,\n",
      "         0.3773,  0.7066, -1.4829,  0.3990], dtype=torch.float64)\n",
      "         x1        x2        x3        x4        x5        x6  cost\n",
      "0  0.374909  0.886411  0.899471  0.580135  0.824566  0.033005   1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([-2.9928], dtype=torch.float64)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = X.generator._create_model(X.algorithm.data)\n",
    "print(model.train_targets)\n",
    "\n",
    "## NOTE: we want to get the minimum evaluated at the highest fidelity -> make sure to use get_recommendation\n",
    "rec = X.generator.get_recommendation(X.algorithm.data)\n",
    "print(rec)\n",
    "problem = AugmentedHartmann(negate=False)\n",
    "problem(torch.tensor(rec.to_numpy())) ## NOTE: the correct global minimum is -3.32237"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "!rm results.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "xopt",
   "language": "python",
   "display_name": "xopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}